{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document classification solution should significantly reduce the manual human effort in the HRM,<br> it should achieve a higher level of accuracy and automation with minimal human intervention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ravindra sharma\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\ravindra sharma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ravindra sharma\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\ravindra sharma\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement matplotlib.tri._triangulation (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlib.tri._triangulation\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ravindra sharma\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\ravindra sharma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ravindra sharma\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\ravindra sharma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ravindra sharma\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\ravindra sharma\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib.tri._triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from functools import reduce\n",
    "from keras.layers import Dense\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from xgboost import XGBClassifier\n",
    "import plotly.graph_objects as go\n",
    "from nltk.corpus import stopwords\n",
    "from lightgbm import LGBMClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataframe which we have made from \"Gathering data.ipynb\" \n",
    "resumes_df=pd.read_csv('resumes_df.csv')\n",
    "resumes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total resumes we have\n",
    "len(resumes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_it(txt):\n",
    "    txt = txt.replace('\\t',' ')\n",
    "    txt = txt.replace('\\n',' ')\n",
    "    \n",
    "    txt = re.sub(r'http\\S+', '', (txt))                                                     # removing links\n",
    "    txt = re.sub('[^a-z ]','', txt.lower())                                                 # only need alphabets also converting to lower case letters for convenience\n",
    "    # txt = txt.split()                           \n",
    "    txt = word_tokenize(txt)                                                                # tokenizing each words\n",
    "    txt = [WordNetLemmatizer().lemmatize(x) for x in txt if x not in stopwords.words() ]    # removing stopwords and using lemmatization for root words\n",
    "    txt = [x for x in txt if len(x.strip())>2]                                              # removing words containing less than 2 letters\n",
    "    \n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning resumes\n",
    "resumes_df['Cleaned_txt'] = resumes_df['Texts'].apply(clean_it)\n",
    "resumes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates if any\n",
    "print('We have',resumes_df.duplicated(subset='Cleaned_txt').sum(),\"duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_df.drop_duplicates(subset='Cleaned_txt',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words we have cleaned \n",
    "\n",
    "sn.histplot(resumes_df['Texts'].apply(lambda x:len(x.split())),label='Before',kde=True,element=\"step\")\n",
    "sn.histplot(resumes_df['Cleaned_txt'].apply(lambda x:len(x.split())),label='After',kde=True,element=\"step\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique categories\n",
    "resumes_df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming them for our convenience\n",
    "resumes_df['Category']=resumes_df['Category'].apply(lambda x: x.split()[0])     # only need the first word  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechecking \n",
    "resumes_df['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking how many resumes we have for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "\n",
    "temp_df=resumes_df['Category'].value_counts()\n",
    "\n",
    "fig.add_pie(labels=temp_df.index,\n",
    "            values=temp_df.values,\n",
    "            hoverinfo='skip',\n",
    "            textinfo='label+value',\n",
    "            textposition='outside')\n",
    "\n",
    "colors = ['lightcyan','cyan','royalblue','darkblue']\n",
    "\n",
    "fig.add_pie(labels=temp_df.index,\n",
    "            values=temp_df.values,\n",
    "            hoverinfo='skip',\n",
    "            marker=dict(colors=colors,\n",
    "                        line=dict(color='#090373',\n",
    "                                  width=0.5)))\n",
    "\n",
    "\n",
    "fig.update_layout(title='<b>Resume counts for each category</b>',\n",
    "                  autosize=False,\n",
    "                  template='plotly_dark',\n",
    "                  showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Document types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Different document types\n",
    "\n",
    "fig=go.Figure()\n",
    "\n",
    "temp_df=resumes_df['Doc Type'].value_counts()\n",
    "\n",
    "colors = ['#1cfc03','#02a80d','#67f090','#3dfc76']\n",
    "\n",
    "fig.add_pie(labels=temp_df.index,\n",
    "            values=temp_df.values,\n",
    "            textinfo='label+value',\n",
    "            pull=[0.1,0.1,0.1],\n",
    "            marker=dict(colors=colors,\n",
    "                        line=dict(color='#03fccf',\n",
    "                                  width=0.5)))\n",
    "\n",
    "\n",
    "fig.update_layout(title='<b>Types of documents we have</b>',\n",
    "                  template='plotly_dark',\n",
    "                  autosize=False,\n",
    "                  showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many urls we have\n",
    "resumes_df['links']=resumes_df['Texts'].apply(lambda x: len(re.findall('http',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "\n",
    "resume_contains_links=len(resumes_df[resumes_df['links']!=0])\n",
    "fig.add_pie(labels=['Contain links','No Links Found'],\n",
    "            values=[resume_contains_links,len(resumes_df)-resume_contains_links],\n",
    "            hole=.5,\n",
    "            pull=[0,0.1],\n",
    "            marker=dict(colors=['#ff9500','#ffd000'][::-1],)\n",
    "            )\n",
    "fig.update_layout(autosize=False,\n",
    "                  template='plotly_dark',\n",
    "                  title='<br>How many resume have links in it<br>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about each resume category [: Average links per category]\n",
    "resumes_df[['links','Category']].groupby('Category').mean().style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to count words\n",
    "\n",
    "def word_counter(series,top=20):\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for string in series:\n",
    "        words.extend(string.split())\n",
    "            \n",
    "    words,counts = np.unique(words,return_counts=True)\n",
    "    \n",
    "    if len(words) <= top:\n",
    "        top = len(words)\n",
    "    \n",
    "    return sorted(zip(words,counts),\n",
    "                  key=lambda x: x[1],\n",
    "                  reverse=True)[:top]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = word_counter(resumes_df['Cleaned_txt'])[::-1]\n",
    "x=[i[1] for i in temp_list]\n",
    "fig = go.Figure()\n",
    "fig.add_bar(y=[i[0] for i in temp_list],\n",
    "            x=x,\n",
    "            text=x,\n",
    "            textposition='outside',\n",
    "            orientation='h',\n",
    "            hovertemplate=\"%{y}<br>%{x}<extra></extra>\",\n",
    "            marker=dict(color=x,\n",
    "                        colorscale='blues'\n",
    "                        ))\n",
    "fig.update_layout(title='<b>Most frequent words</b>',\n",
    "                  xaxis_title=\"Count\",\n",
    "                  template='simple_white',\n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking the same for each category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels= resumes_df['Category'].unique()\n",
    "\n",
    "fig=make_subplots(rows = 2, cols = 2,\n",
    "                  subplot_titles = cat_labels,\n",
    "                  horizontal_spacing=0.1,\n",
    "                  vertical_spacing=0.1\n",
    "                  )\n",
    "\n",
    "row=1\n",
    "col=1\n",
    "\n",
    "for i in cat_labels:\n",
    "    \n",
    "    temp_list = word_counter(resumes_df[resumes_df['Category']==i]['Cleaned_txt'],15)[::-1]\n",
    "    x=[i[1] for i in temp_list]\n",
    "    \n",
    "    fig.add_bar(y=[i[0] for i in temp_list],\n",
    "            x=x,\n",
    "            text=x,\n",
    "            textposition='outside',\n",
    "            orientation='h',\n",
    "            hovertemplate=\"%{y}<br>%{x}<extra></extra>\",\n",
    "            marker=dict(color=x,\n",
    "                        colorscale='Plotly3'\n",
    "                        ),\n",
    "            row=row,\n",
    "            col=col)\n",
    "    \n",
    "    fig.update_layout(xaxis=dict(showline=False,))\n",
    "    \n",
    "    \n",
    "    col+=1\n",
    "    \n",
    "    if col>2:\n",
    "        col=1\n",
    "        row+=1\n",
    "    \n",
    "fig.update_layout(title='<b>Frequent words for each class</b>',\n",
    "                  showlegend=False,\n",
    "                  template='simple_white',\n",
    "                  height=700,)\n",
    "\n",
    "for i in range(1,5):\n",
    "    fig.update_layout(**{f'xaxis{i}':dict(showline=False,\n",
    "                                          showticklabels=False,\n",
    "                                          ticks=\"\"\n",
    "                                          )})\n",
    "        \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In different resumes same types of words are repeated.<br>\n",
    "You must add your 'Experience' to your resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Work cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=reduce(lambda x,y:x+' '+y, resumes_df['Cleaned_txt'])         # making all resumes to single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(WordCloud(width = 1400, \n",
    "                    height = 800,).generate(text))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Project\" and \"Experience\" are the most in resume.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking for individual class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "for n,i in enumerate(cat_labels):\n",
    "    \n",
    "    text=reduce(lambda x,y:x+' '+y, resumes_df[resumes_df['Category']==i]['Cleaned_txt'])\n",
    "    \n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.imshow(WordCloud().generate(text))\n",
    "    plt.axis('off')\n",
    "    plt.title(i)\n",
    "\n",
    "plt.suptitle('Word Cloud For Each Class',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking parts of speech\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df=pd.DataFrame(columns=cat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(cat_labels):\n",
    "    \n",
    "    text=reduce(lambda x,y:x+' '+y, resumes_df[resumes_df['Category']==j]['Texts'])\n",
    "    for i in nlp(text):\n",
    "        if i.pos_ in pos_df.index:\n",
    "            if np.isnan(pos_df.loc[i.pos_,j]):\n",
    "                pos_df.loc[i.pos_,j]=1\n",
    "            else:\n",
    "                pos_df.loc[i.pos_,j] += 1\n",
    "        else:\n",
    "            pos_df.loc[i.pos_,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sn.heatmap(pos_df.dropna().astype('int').T,\n",
    "           annot=True,\n",
    "           fmt='.0f',\n",
    "           cbar=False,\n",
    "           linewidths=2,\n",
    "           cmap='Purples'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "for i in pos_df.columns:\n",
    "    fig.add_scatter(x=pos_df.index,\n",
    "                    y=pos_df[i],\n",
    "                    name=i)\n",
    "fig.update_layout(template=\"plotly_dark\",\n",
    "                  hovermode='x unified',\n",
    "                  title=\"Parts Of Speech count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun, Pronoun and verbs are used More."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_count_df=resumes_df.iloc[:,[1,-2]].groupby('Category').agg(lambda x: len(x.sum().split())/len(x)).sort_values(by='Cleaned_txt',ascending=False)\n",
    "avg_word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Funnel(x=avg_word_count_df.Cleaned_txt.astype('int'),\n",
    "                          y=avg_word_count_df.index,\n",
    "                          marker = {\"color\": [\"deepskyblue\", \"lightsalmon\", \"tan\", \"teal\"],\n",
    "                                    \"line\": {\"width\": [4, 2, 2, 3, 1],\n",
    "                                             \"color\": [\"wheat\", \"wheat\", \"blue\", \"wheat\",]}},\n",
    "                          connector = {\"line\": {\"color\": \"royalblue\",\n",
    "                                                \"dash\": \"dot\",\n",
    "                                                \"width\": 3}\n",
    "                                       }))\n",
    "\n",
    "fig.update_layout(autosize=False,\n",
    "                  title=\"<b>Average word count in each resume type</b>\",\n",
    "                  template=\"plotly_dark\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before model building we need to vactorize our text so as to feed algorithms.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tfidf vectorizer\n",
    "tfidf= TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf will return a sparse matrix so converting them to array\n",
    "vectorized_x=tfidf.fit_transform(resumes_df['Cleaned_txt']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are in string data type\n",
    "# encoding them\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying on category column\n",
    "target_y=le.fit_transform(resumes_df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sn.heatmap(pd.DataFrame(vectorized_x).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our data is imbalance, we have to use stratified techniques\n",
    "cv=RepeatedStratifiedKFold(n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_accuracy(x,y,model,indicator=False):\n",
    "\n",
    "    train_scores=[]                                                     # To store scores of each folds(subset of data)\n",
    "    test_scores=[]  \n",
    "    \n",
    "    for train_idx, test_idx in tqdm(cv.split(x, y)):                          # cv.split will return index of folds\n",
    "        \n",
    "        x_tr, y_tr = x[train_idx], y[train_idx]                         # now storing values to a new train and test set\n",
    "        x_te, y_te = x[test_idx], y[test_idx]\n",
    "        \n",
    "        model.fit(x_tr, y_tr)                                           # fitted training set\n",
    "        \n",
    "        train_scores.append(accuracy_score(y_tr,\n",
    "                                            model.predict(x_tr)))\n",
    "        test_scores.append(accuracy_score(y_te,\n",
    "                                            model.predict(x_te)))\n",
    "\n",
    "    avg_train_scores=np.mean(train_scores)                              # Storing average accuracy\n",
    "    avg_test_scores=np.mean(test_scores)\n",
    "    \n",
    "    \n",
    "    if indicator:\n",
    "        \n",
    "        fig = go.Figure()\n",
    "\n",
    "        common_attributes=dict(\n",
    "                                gauge={\n",
    "                                    'axis': {'range': [None, 1]},\n",
    "                                    'threshold': {'line': {'color': \"red\", \n",
    "                                                            'width': 4},\n",
    "                                                    'thickness': 0.75,\n",
    "                                                    'value': 1},\n",
    "                                    'bgcolor': \"white\",\n",
    "                                    'bar': {'color': \"#0406b0\"},\n",
    "                                    'steps': [\n",
    "                                                {'range': [0, 0.7], 'color': '#fc9e5b'},\n",
    "                                                {'range': [0.7, 0.9], 'color': '#99fc5b'}]\n",
    "                                            },\n",
    "                                mode = \"number+gauge+delta\",\n",
    "                                delta={'reference': 1},\n",
    "                                )\n",
    "        fig.add_indicator(\n",
    "                        **common_attributes,\n",
    "                        value = avg_train_scores,\n",
    "                        title = {'text': \"Train Accuuracy\"},\n",
    "                        domain = {'x': [0,0.5], 'y': [0, 1]}\n",
    "                        )\n",
    "        fig.add_indicator(**common_attributes,\n",
    "                        value = avg_test_scores,\n",
    "                        title = {'text': \"Test Accuuracy\"},\n",
    "                        domain = {'x': [0.5,1], 'y': [0, 1]}\n",
    "                        )\n",
    "\n",
    "        fig.update_layout(paper_bgcolor = \"lavender\", font = {'color': \"darkblue\", 'family': \"Arial\"})\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    \n",
    "    return avg_train_scores,avg_test_scores                                # mean of accuracies we got\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To vrify model breifly\n",
    "\n",
    "def check_my_score(x,y,model):\n",
    "    \"\"\"Gives confussion_matrix and classification_report for training and testing set.\n",
    "\n",
    "    Args:\n",
    "        model : takes a defined model\n",
    "    \"\"\"\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,\n",
    "                                                     test_size= 0.2,\n",
    "                                                     random_state=1,\n",
    "                                                     stratify=y)\n",
    "    train_cm=confusion_matrix(y_train,model.predict(x_train))\n",
    "    test_cm=confusion_matrix(y_test,model.predict(x_test))\n",
    "\n",
    "    # plotting confusion metrics\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1,title='Train set')\n",
    "    sn.heatmap(train_cm,annot=True,fmt=\".0f\",cmap='cool',cbar=False,)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.subplot(1,2,2,title='Test set')\n",
    "    sn.heatmap(test_cm,annot=True,fmt=\".0f\",cmap='icefire',cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "    # Classification reports\n",
    "    print('-'*17+'\\nOn training set :\\n'+'-'*17,'\\n',classification_report(y_train,model.predict(x_train)))\n",
    "    print('-'*16+'\\nOn testing set :\\n'+'-'*16,'\\n',classification_report(y_test,model.predict(x_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best model\n",
    "def find_best_algo(x, y,return_frame=False):\n",
    "    \n",
    "    # initiating a dictionary\n",
    "    models = {}\n",
    "    \n",
    "    # Storing model_names and model\n",
    "    models['Logistic Regression']=LogisticRegression()                      # Logistic Regression\n",
    "    models['KNN']=KNeighborsClassifier()                                    # KNN Classifier\n",
    "    models['SVC']=SVC()                                                     # Support Vector Classifier\n",
    "    models['Naive Bayes']=MultinomialNB()                                   # Naive Bayes Classifier\n",
    "    models['Decision Tree']=DecisionTreeClassifier()                        # Decision Tree Classifier\n",
    "    models['RandomForest']=RandomForestClassifier()                         # Random Forest Classifier\n",
    "    models['Gradient Boosting']=GradientBoostingClassifier()                # Gradient Boosting Classifier\n",
    "    models['AdaBoost']=AdaBoostClassifier()                                 # AdaBoost Classifier\n",
    "    models['LightGBM']=LGBMClassifier()                                     # LightGBM Classifier\n",
    "    models['XGBoost']=XGBClassifier()                                       # XGBoost Classifier\n",
    "    models['Catboost']=CatBoostClassifier(verbose=0,                        # Catboost Classifier\n",
    "                                          allow_writing_files=False)\n",
    "    \n",
    "    \n",
    "    # DataFrane to store model results\n",
    "    model_scores_df= pd.DataFrame(columns=['accuracy_train','accuracy_test'] )\n",
    "    \n",
    "    # Now iterating over each model\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        try:\n",
    "            model.set_params(random_state=1)                                # Blocking randomness\n",
    "        except:                                                             # Some of them are not supported\n",
    "            pass\n",
    "        \n",
    "        # storing scores\n",
    "        model_scores_df.loc[model_name]=train_test_accuracy(x,y,model,indicator=False)                                         \n",
    "       \n",
    "    \n",
    "    \n",
    "    if return_frame:\n",
    "        return model_scores_df\n",
    "    \n",
    "    \n",
    "    # The plot\n",
    "    fig=go.Figure()\n",
    "    x=model_scores_df.index\n",
    "    \n",
    "    colors=['#03d1ff','#00fc08']\n",
    "    \n",
    "    for i in range(2):\n",
    "        \n",
    "        fig.add_scatter(x=x,\n",
    "                        y=model_scores_df.iloc[:,i],\n",
    "                        marker=dict(color=colors[i]),\n",
    "                        name=model_scores_df.columns[i])\n",
    "        \n",
    "    fig.update_layout(hovermode='x unified',\n",
    "                    template='plotly_dark',\n",
    "                    title='Comapring models')\n",
    "    return fig\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding which algorithm is the best\n",
    "find_best_algo(vectorized_x, target_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except 'CatBoostClassifier', none of them performed well.<br>\n",
    "It that because of our data is imbalanced.<br>\n",
    "Lets find out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SMOTE to oversample our data\n",
    "\n",
    "smote=SMOTE(random_state=1)\n",
    "\n",
    "resampled_x,resampled_y=smote.fit_resample(vectorized_x, target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution before and after over_sampling\n",
    "\n",
    "before_sample=np.unique(target_y,return_counts=True)\n",
    "after_sample=np.unique(resampled_y,return_counts=True)\n",
    "\n",
    "pull=[0.01 for i in range(4)]\n",
    "x=le.inverse_transform(before_sample[0])\n",
    "fig=go.Figure()\n",
    "fig.add_pie(labels=x,\n",
    "            values=before_sample[1],\n",
    "            name='Before',\n",
    "            hole=0.3,\n",
    "            sort=False,\n",
    "            pull=pull,\n",
    "            showlegend=False,\n",
    "            marker=dict(colors=['#b1e84a','#8ffa48','#63f205','#4bb804']),\n",
    "            )\n",
    "fig.add_pie(labels=x,\n",
    "            values=after_sample[1],\n",
    "            name='After',\n",
    "            hole=0.6,\n",
    "            pull=pull,\n",
    "            showlegend=False,\n",
    "            marker=dict(colors=['#ffff03','#ffb200','#ff7b00','#cc543b']),\n",
    "            )\n",
    "fig.update_layout(autosize=False,\n",
    "                  title='Distribution of target variable before and after sampling',\n",
    "                  template='plotly_dark',\n",
    "                  showlegend=False,)\n",
    "\n",
    "fig.data[0].domain = {'x': [0,1], 'y': [0.25,0.75] }\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again checking our Algos score\n",
    "find_best_algo(resampled_x,resampled_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After over sampling, model's performance increased significantly.<br>\n",
    "Naive Bayes is performing so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing accuracies before sampling\n",
    "train_test_accuracy(vectorized_x, target_y,mnb,indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing accuracies After sampling\n",
    "train_test_accuracy(resampled_x,resampled_y,mnb,indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking precision and recall\n",
    "check_my_score(resampled_x,resampled_y,mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(n_jobs=-1,\n",
    "                      multi_class='multinomial',\n",
    "                      random_state=1,\n",
    "                      solver='saga',\n",
    "                      max_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params={'C':(1e-6, 100.0, 'log-uniform'),\n",
    "               'penalty':['elasticnet'],\n",
    "               'l1_ratio':np.linspace(0,1,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = BayesSearchCV(estimator=lr,\n",
    "                       search_spaces=search_params,\n",
    "                       n_jobs=-1,\n",
    "                       cv=cv,\n",
    "                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(vectorized_x,target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.set_params(**search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_accuracy(vectorized_x,target_y,lr,indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without oversampling, 100% accuracy can be achievable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_my_score(vectorized_x,target_y,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'p':[1,2],\n",
    "            'n_neighbors':range(2,10),\n",
    "            'weights':['uniform', 'distance'],\n",
    "            'algorithm':['ball_tree', 'kd_tree', 'brute',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_knn = GridSearchCV(knn,\n",
    "                        cv=cv,\n",
    "                        n_jobs=-1,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='accuracy',\n",
    "                        return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_knn.fit(vectorized_x,target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.set_params(**gscv_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_accuracy(vectorized_x,target_y,knn,indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_my_score(vectorized_x,target_y,knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=pd.read_csv(\"validation_df.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding our categories  \n",
    "validation_y=le.transform(validation_df.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['Cleaned Text']=validation_df.Texts.apply(clean_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_x=tfidf.transform(validation_df['Cleaned Text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store all models prediction\n",
    "model_preds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds['Actual']=validation_y\n",
    "\n",
    "model_preds['Naive Bayes']=mnb.predict(validation_x)\n",
    "model_preds['Logistic Regression']=lr.predict(validation_x)\n",
    "model_preds['K-Nearest Neighbour']=knn.predict(validation_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_preds).style.background_gradient(cmap='Set2',subset=['Actual']).background_gradient(cmap='Pastel2',subset=['Naive Bayes','Logistic Regression','K-Nearest Neighbour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these algorithms are predicted correctly.<br> But which one to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a model which take less time to predict\n",
    "\n",
    "time_takes = []\n",
    "\n",
    "for i in [mnb,lr,knn]:\n",
    "    \n",
    "    temp_time_store=0\n",
    "    \n",
    "    for _ in range(3):\n",
    "        start = time()\n",
    "        i.predict(np.vstack((vectorized_x,validation_x)))\n",
    "        temp_time_store+= time()-start\n",
    "        \n",
    "    time_takes.append(temp_time_store/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "\n",
    "fig.add_bar(x=['Naive Bayes','Logistic Regression','K-Nearest Neighbour'],\n",
    "            y=time_takes,\n",
    "            marker=dict(color=time_takes,\n",
    "                        colorscale='rainbow'),\n",
    "            text=np.round(time_takes,4),\n",
    "            hovertemplate='%{x}<br>%{y}<extra></extra>',\n",
    "            textposition='outside',\n",
    "            width=0.2\n",
    "            )\n",
    "\n",
    "fig.update_layout(title='Time Taken by Algorithm',\n",
    "                  xaxis_title='Algorithm',\n",
    "                  yaxis_title='Time Taken (s)',\n",
    "                  template='plotly_dark',\n",
    "                  autosize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower is the better, hence 'Logistic Regression' is the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peparation for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class that wraps your function\n",
    "class MyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, func=clean_it):\n",
    "        self.func = func\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('my_transformer', MyTransformer()),\n",
    "    ('vectorize',TfidfVectorizer()),\n",
    "    ('model',LogisticRegression(n_jobs=-1,\n",
    "                                C= 10.507056203697442,\n",
    "                                l1_ratio=0.3333333333333333,\n",
    "                                penalty='elasticnet',\n",
    "                                multi_class='multinomial',\n",
    "                                random_state=1,\n",
    "                                solver='saga',\n",
    "                                max_iter=100000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(resumes_df['Texts'],resumes_df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(validation_df['Texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a pickle file\n",
    "pickle.dump(pipeline,open('pipeline.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
